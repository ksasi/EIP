{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K70hAckqg0EA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.backend import tf as ktf\n",
    "from  skimage import transform\n",
    "import numpy as np\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06C9kT6weACi"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(112)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(221)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKlUWrcIUtma"
   },
   "outputs": [],
   "source": [
    "TF_ENABLE_WINOGRAD_NONFUSED=1\n",
    "!export TF_ENABLE_WINOGRAD_NONFUSED=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "#batch_size = 128\n",
    "batch_size = 512  # --- Since image size is 16x16, and applicable on V100\n",
    "num_classes = 10\n",
    "#epochs = 20\n",
    "epochs = 75\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0_rzO9DUtmf"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "36e4d1bd-a5f3-4235-98ef-8488f9865bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 18s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "mean_cifar10 = [125.3, 123.0, 113.9]\n",
    "std_cifar10  = [63.0,  62.1,  66.7]\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#Perchannel normalization\n",
    "def defnormalize_image_by_chanel(img):\n",
    "    new_img = np.zeros(img.shape)\n",
    "    mean_cifar10 = [125.3, 123.0, 113.9]\n",
    "    std_cifar10  = [63.0,  62.1,  66.7]\n",
    "    for i in range(3):\n",
    "        new_img[:, :, i] = (img[:, :, i] - mean_cifar10[i]) / std_cifar10[i]\n",
    "    return new_img\n",
    "\n",
    "x_train = np.asarray([defnormalize_image_by_chanel(image) for image in x_train])\n",
    "x_test = np.asarray([defnormalize_image_by_chanel(image) for image in x_test])\n",
    "\n",
    "\n",
    "#Reducing the size of the images to 16x16 initially\n",
    "new_shape = (16,16,3)\n",
    "x_train = np.asarray([transform.resize(image, new_shape) for image in x_train])\n",
    "x_test = np.asarray([transform.resize(image, new_shape) for image in x_test])\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9FQ6GevmgoTy",
    "outputId": "f32467fd-fa8c-4de3-b1fd-9ddaed1ccd97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 16, 16, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "citkbTKL-0Gd"
   },
   "outputs": [],
   "source": [
    "\n",
    "datagen_bm = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=5./32,\n",
    "    height_shift_range=5./32,\n",
    "    horizontal_flip=True)\n",
    "                    \n",
    "datagen_k = MixupGenerator(x_train, y_train,  batch_size=batch_size, alpha=0.3, datagen=datagen_bm)()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    #flat = Flatten()(AvgPooling)\n",
    "    flat = GlobalAveragePooling2D()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_filter = 22\n",
    "dropout_rate = 0.2\n",
    "l = 20\n",
    "##l = 44\n",
    "##num_filter = 11\n",
    "##input = Input(shape=(img_height, img_width, channel,))\n",
    "input = Input(shape=(None, None, channel,))\n",
    "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kFh7pdxhNtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 2 594         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 2 88          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 2 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 1 2178        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, None, 1 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 3 0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 132         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 3267        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 4 176         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 4 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 4356        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, None, 1 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 5 0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 5 220         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 5 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 5445        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, None, 1 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, None, 6 0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 6 264         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 6534        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, None, 1 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, None, None, 7 0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 7 308         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 7 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 7623        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, None, 1 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, None, None, 8 0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 8 352         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 8 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 1 8712        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, None, 1 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, None, None, 9 0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 9 396         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 9 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 9801        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, None, None, 1 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, None, None, 1 0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 1 440         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 1 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 10890       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, None, None, 1 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, None, 1 0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 1 484         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 1 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 11979       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, None, 1 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, None, 1 0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 1 528         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 1 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 1 13068       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, None, None, 1 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, None, 1 0           concatenate_10[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 1 572         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 1 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 14157       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, None, None, 1 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, None, None, 1 0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 1 616         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 1 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 1 15246       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, None, None, 1 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, None, None, 1 0           concatenate_12[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 1 660         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 1 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 1 16335       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, None, None, 1 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, None, None, 1 0           concatenate_13[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 1 704         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 1 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 1 17424       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, None, None, 1 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, None, None, 1 0           concatenate_14[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 1 748         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 1 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 1 18513       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, None, None, 1 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, None, None, 1 0           concatenate_15[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 1 792         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 1 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 1 19602       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, None, None, 1 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, None, None, 2 0           concatenate_16[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 2 836         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 2 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 20691       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, None, None, 1 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, None, None, 2 0           concatenate_17[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 2 880         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 2 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 1 21780       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, None, None, 1 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, None, None, 2 0           concatenate_18[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 2 924         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 2 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 1 22869       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, None, None, 1 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, None, None, 2 0           concatenate_19[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 2 968         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 2 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 1 2662        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, None, None, 1 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 1 0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 1 44          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 1 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 1 1089        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, None, None, 1 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, None, None, 2 0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 2 88          concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 2 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 1 2178        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, None, None, 1 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, None, None, 3 0           concatenate_21[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 3 132         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 3 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 1 3267        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, None, None, 1 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, None, None, 4 0           concatenate_22[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 4 176         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 4 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 1 4356        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, None, None, 1 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, None, None, 5 0           concatenate_23[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 5 220         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 5 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 1 5445        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, None, None, 1 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, None, None, 6 0           concatenate_24[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 6 264         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 6 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 1 6534        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, None, None, 1 0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, None, None, 7 0           concatenate_25[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 7 308         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 7 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 1 7623        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, None, None, 1 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, None, None, 8 0           concatenate_26[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 8 352         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 8 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 1 8712        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, None, None, 1 0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, None, None, 9 0           concatenate_27[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 9 396         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 9 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 1 9801        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, None, None, 1 0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, None, None, 1 0           concatenate_28[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 1 440         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 1 10890       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, None, None, 1 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, None, None, 1 0           concatenate_29[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 1 484         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 1 11979       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, None, None, 1 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, None, None, 1 0           concatenate_30[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 1 528         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 1 13068       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, None, None, 1 0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, None, None, 1 0           concatenate_31[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 1 572         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 1 14157       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, None, None, 1 0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, None, None, 1 0           concatenate_32[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 1 616         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 1 15246       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, None, None, 1 0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, None, None, 1 0           concatenate_33[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 1 660         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 1 16335       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, None, None, 1 0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, None, None, 1 0           concatenate_34[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 1 704         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 1 17424       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, None, None, 1 0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, None, None, 1 0           concatenate_35[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 1 748         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 1 18513       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, None, None, 1 0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, None, None, 1 0           concatenate_36[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 1 792         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 1 19602       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, None, None, 1 0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, None, None, 2 0           concatenate_37[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 2 836         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 2 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 1 20691       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, None, None, 1 0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, None, None, 2 0           concatenate_38[0][0]             \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 2 880         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 2 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 1 21780       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, None, None, 1 0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, None, None, 2 0           concatenate_39[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 2 924         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 2 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 2541        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, None, None, 1 0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, None, None, 1 0           dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 1 44          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 1089        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, None, None, 1 0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, None, None, 2 0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 2 88          concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 2 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 1 2178        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, None, None, 1 0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, None, None, 3 0           concatenate_41[0][0]             \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 3 132         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 3 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 3267        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, None, None, 1 0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, None, None, 4 0           concatenate_42[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 4 176         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 4 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 4356        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, None, None, 1 0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, None, None, 5 0           concatenate_43[0][0]             \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 5 220         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 5 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 5445        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, None, None, 1 0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, None, None, 6 0           concatenate_44[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 6 264         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 6 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 6534        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, None, None, 1 0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, None, None, 7 0           concatenate_45[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 7 308         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 7 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 7623        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, None, None, 1 0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, None, None, 8 0           concatenate_46[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 8 352         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, None, None, 8 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 8712        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, None, None, 1 0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, None, None, 9 0           concatenate_47[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 9 396         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, None, None, 9 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 9801        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, None, None, 1 0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, None, None, 1 0           concatenate_48[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 440         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 10890       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, None, None, 1 0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, None, None, 1 0           concatenate_49[0][0]             \n",
      "                                                                 dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 1 484         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 11979       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, None, None, 1 0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, None, None, 1 0           concatenate_50[0][0]             \n",
      "                                                                 dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 528         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 1 13068       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, None, None, 1 0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, None, None, 1 0           concatenate_51[0][0]             \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 1 572         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 14157       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, None, None, 1 0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, None, None, 1 0           concatenate_52[0][0]             \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 616         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 15246       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, None, None, 1 0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, None, None, 1 0           concatenate_53[0][0]             \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 1 660         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 16335       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, None, None, 1 0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, None, None, 1 0           concatenate_54[0][0]             \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 704         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 1 17424       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, None, None, 1 0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, None, None, 1 0           concatenate_55[0][0]             \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 1 748         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 1 18513       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, None, None, 1 0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, None, None, 1 0           concatenate_56[0][0]             \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 1 792         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 1 19602       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, None, None, 1 0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, None, None, 2 0           concatenate_57[0][0]             \n",
      "                                                                 dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 2 836         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, None, 2 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 1 20691       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, None, None, 1 0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, None, None, 2 0           concatenate_58[0][0]             \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 2 880         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, None, 2 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 1 21780       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, None, None, 1 0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, None, None, 2 0           concatenate_59[0][0]             \n",
      "                                                                 dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 2 924         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, None, 2 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 1 2541        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, None, None, 1 0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, None, None, 1 0           dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 1 44          average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 1 1089        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, None, None, 1 0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, None, None, 2 0           average_pooling2d_3[0][0]        \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 2 88          concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, None, 2 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 1 2178        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, None, None, 1 0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, None, None, 3 0           concatenate_61[0][0]             \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 3 132         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, None, 3 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 1 3267        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, None, None, 1 0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, None, None, 4 0           concatenate_62[0][0]             \n",
      "                                                                 dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 4 176         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, None, 4 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 4356        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, None, None, 1 0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, None, None, 5 0           concatenate_63[0][0]             \n",
      "                                                                 dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 5 220         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, None, 5 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 5445        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, None, None, 1 0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, None, None, 6 0           concatenate_64[0][0]             \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 6 264         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, None, 6 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 1 6534        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, None, None, 1 0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, None, None, 7 0           concatenate_65[0][0]             \n",
      "                                                                 dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 7 308         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 7 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 1 7623        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, None, None, 1 0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, None, None, 8 0           concatenate_66[0][0]             \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 8 352         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 8 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 1 8712        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, None, None, 1 0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, None, None, 9 0           concatenate_67[0][0]             \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 9 396         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 9 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 9801        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, None, None, 1 0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, None, None, 1 0           concatenate_68[0][0]             \n",
      "                                                                 dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, None, 1 440         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 1 10890       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, None, None, 1 0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, None, None, 1 0           concatenate_69[0][0]             \n",
      "                                                                 dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 1 484         concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 1 11979       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, None, None, 1 0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, None, None, 1 0           concatenate_70[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 1 528         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 1 13068       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, None, None, 1 0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, None, None, 1 0           concatenate_71[0][0]             \n",
      "                                                                 dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 1 572         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 1 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 1 14157       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, None, None, 1 0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, None, None, 1 0           concatenate_72[0][0]             \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 1 616         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 1 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 1 15246       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, None, None, 1 0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, None, None, 1 0           concatenate_73[0][0]             \n",
      "                                                                 dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 1 660         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 1 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 1 16335       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, None, None, 1 0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, None, None, 1 0           concatenate_74[0][0]             \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 1 704         concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 1 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 1 17424       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, None, None, 1 0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, None, None, 1 0           concatenate_75[0][0]             \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 1 748         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 1 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 1 18513       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, None, None, 1 0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, None, None, 1 0           concatenate_76[0][0]             \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 1 792         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 1 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 1 19602       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, None, None, 1 0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, None, None, 2 0           concatenate_77[0][0]             \n",
      "                                                                 dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 2 836         concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 2 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 1 20691       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, None, None, 1 0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, None, None, 2 0           concatenate_78[0][0]             \n",
      "                                                                 dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 2 880         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 2 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 1 21780       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, None, None, 1 0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, None, None, 2 0           concatenate_79[0][0]             \n",
      "                                                                 dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 2 924         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, None, 2 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, None, None, 2 0           activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 231)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2320        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 988,778\n",
      "Trainable params: 967,988\n",
      "Non-trainable params: 20,790\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1771
    },
    "colab_type": "code",
    "id": "crhGk7kEhXAz",
    "outputId": "251a8802-9c27-45e9-fa1d-b672dddb1165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "97/97 [==============================] - 86s 885ms/step - loss: 2.0327 - acc: 0.2550 - val_loss: 2.0984 - val_acc: 0.3350\n",
      "Epoch 2/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.8337 - acc: 0.3617 - val_loss: 3.8584 - val_acc: 0.2277\n",
      "Epoch 3/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.7270 - acc: 0.4143 - val_loss: 2.7394 - val_acc: 0.3441\n",
      "Epoch 4/75\n",
      "97/97 [==============================] - 60s 618ms/step - loss: 1.6380 - acc: 0.4606 - val_loss: 3.1778 - val_acc: 0.3622\n",
      "Epoch 5/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.5763 - acc: 0.4934 - val_loss: 1.8862 - val_acc: 0.4646\n",
      "Epoch 6/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.5175 - acc: 0.5229 - val_loss: 1.5620 - val_acc: 0.5356\n",
      "Epoch 7/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.4698 - acc: 0.5445 - val_loss: 1.5679 - val_acc: 0.5408\n",
      "Epoch 8/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.4297 - acc: 0.5615 - val_loss: 1.7346 - val_acc: 0.5463\n",
      "Epoch 9/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.4026 - acc: 0.5765 - val_loss: 2.0325 - val_acc: 0.4885\n",
      "Epoch 10/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.3810 - acc: 0.5873 - val_loss: 1.7879 - val_acc: 0.5192\n",
      "Epoch 11/75\n",
      "97/97 [==============================] - 60s 618ms/step - loss: 1.3588 - acc: 0.5965 - val_loss: 1.5219 - val_acc: 0.5707\n",
      "Epoch 12/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.3421 - acc: 0.6042 - val_loss: 1.3114 - val_acc: 0.5985\n",
      "Epoch 13/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.3134 - acc: 0.6185 - val_loss: 1.9697 - val_acc: 0.5191\n",
      "Epoch 14/75\n",
      "97/97 [==============================] - 60s 618ms/step - loss: 1.2953 - acc: 0.6268 - val_loss: 1.6525 - val_acc: 0.5743\n",
      "Epoch 15/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2962 - acc: 0.6296 - val_loss: 1.5373 - val_acc: 0.5793\n",
      "Epoch 16/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2775 - acc: 0.6408 - val_loss: 1.6832 - val_acc: 0.5785\n",
      "Epoch 17/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2688 - acc: 0.6423 - val_loss: 1.2153 - val_acc: 0.6492\n",
      "Epoch 18/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.2494 - acc: 0.6523 - val_loss: 1.3210 - val_acc: 0.6222\n",
      "Epoch 19/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2425 - acc: 0.6558 - val_loss: 1.4297 - val_acc: 0.6042\n",
      "Epoch 20/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.2361 - acc: 0.6583 - val_loss: 1.3014 - val_acc: 0.6170\n",
      "Epoch 21/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2309 - acc: 0.6595 - val_loss: 1.5208 - val_acc: 0.5974\n",
      "Epoch 22/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2142 - acc: 0.6685 - val_loss: 1.8470 - val_acc: 0.5596\n",
      "Epoch 23/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.2046 - acc: 0.6730 - val_loss: 1.2824 - val_acc: 0.6600\n",
      "Epoch 24/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.2036 - acc: 0.6776 - val_loss: 0.9214 - val_acc: 0.7085\n",
      "Epoch 25/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1957 - acc: 0.6813 - val_loss: 1.1299 - val_acc: 0.6689\n",
      "Epoch 26/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1960 - acc: 0.6812 - val_loss: 0.9564 - val_acc: 0.7113\n",
      "Epoch 27/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1844 - acc: 0.6845 - val_loss: 1.2321 - val_acc: 0.6541\n",
      "Epoch 28/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1714 - acc: 0.6879 - val_loss: 1.1179 - val_acc: 0.6808\n",
      "Epoch 29/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1735 - acc: 0.6897 - val_loss: 1.1179 - val_acc: 0.6760\n",
      "Epoch 30/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1640 - acc: 0.6945 - val_loss: 0.8230 - val_acc: 0.7442\n",
      "Epoch 31/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1556 - acc: 0.6982 - val_loss: 1.1983 - val_acc: 0.6619\n",
      "Epoch 32/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1608 - acc: 0.6952 - val_loss: 1.1723 - val_acc: 0.6767\n",
      "Epoch 33/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1642 - acc: 0.6957 - val_loss: 1.5788 - val_acc: 0.6294\n",
      "Epoch 34/75\n",
      "97/97 [==============================] - 60s 621ms/step - loss: 1.1480 - acc: 0.6999 - val_loss: 0.9158 - val_acc: 0.7320\n",
      "Epoch 35/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1447 - acc: 0.7021 - val_loss: 1.3582 - val_acc: 0.6435\n",
      "Epoch 36/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1480 - acc: 0.7016 - val_loss: 0.8360 - val_acc: 0.7341\n",
      "Epoch 37/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1429 - acc: 0.7080 - val_loss: 0.9619 - val_acc: 0.7021\n",
      "Epoch 38/75\n",
      "97/97 [==============================] - 60s 621ms/step - loss: 1.1387 - acc: 0.7090 - val_loss: 0.8013 - val_acc: 0.7565\n",
      "Epoch 39/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.1325 - acc: 0.7096 - val_loss: 0.9337 - val_acc: 0.7237\n",
      "Epoch 40/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1243 - acc: 0.7120 - val_loss: 0.8159 - val_acc: 0.7433\n",
      "Epoch 41/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1050 - acc: 0.7216 - val_loss: 0.7624 - val_acc: 0.7631\n",
      "Epoch 42/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.1037 - acc: 0.7207 - val_loss: 0.7045 - val_acc: 0.7786\n",
      "Epoch 43/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0966 - acc: 0.7232 - val_loss: 0.8191 - val_acc: 0.7601\n",
      "Epoch 44/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.0897 - acc: 0.7278 - val_loss: 0.9362 - val_acc: 0.7120\n",
      "Epoch 45/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0804 - acc: 0.7291 - val_loss: 0.9336 - val_acc: 0.7344\n",
      "Epoch 46/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0754 - acc: 0.7343 - val_loss: 0.6163 - val_acc: 0.8053\n",
      "Epoch 47/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0723 - acc: 0.7348 - val_loss: 0.7324 - val_acc: 0.7681\n",
      "Epoch 48/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0568 - acc: 0.7380 - val_loss: 0.6806 - val_acc: 0.7892\n",
      "Epoch 49/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0603 - acc: 0.7369 - val_loss: 0.8276 - val_acc: 0.7500\n",
      "Epoch 50/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0565 - acc: 0.7404 - val_loss: 0.6623 - val_acc: 0.7988\n",
      "Epoch 51/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0473 - acc: 0.7405 - val_loss: 0.6974 - val_acc: 0.7820\n",
      "Epoch 52/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0429 - acc: 0.7437 - val_loss: 0.7489 - val_acc: 0.7713\n",
      "Epoch 53/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0355 - acc: 0.7514 - val_loss: 0.6620 - val_acc: 0.7933\n",
      "Epoch 54/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0308 - acc: 0.7508 - val_loss: 0.7822 - val_acc: 0.7655\n",
      "Epoch 55/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0211 - acc: 0.7535 - val_loss: 0.7635 - val_acc: 0.7705\n",
      "Epoch 56/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0166 - acc: 0.7562 - val_loss: 0.6134 - val_acc: 0.8031\n",
      "Epoch 57/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0116 - acc: 0.7591 - val_loss: 0.6117 - val_acc: 0.8074\n",
      "Epoch 58/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 1.0086 - acc: 0.7582 - val_loss: 0.5743 - val_acc: 0.8207\n",
      "Epoch 59/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0057 - acc: 0.7605 - val_loss: 0.6426 - val_acc: 0.7983\n",
      "Epoch 60/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 1.0105 - acc: 0.7601 - val_loss: 0.6069 - val_acc: 0.8033\n",
      "Epoch 61/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9923 - acc: 0.7651 - val_loss: 0.6622 - val_acc: 0.7920\n",
      "Epoch 62/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9873 - acc: 0.7682 - val_loss: 0.6363 - val_acc: 0.8007\n",
      "Epoch 63/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9863 - acc: 0.7671 - val_loss: 0.6543 - val_acc: 0.7952\n",
      "Epoch 64/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 0.9845 - acc: 0.7687 - val_loss: 0.6384 - val_acc: 0.7989\n",
      "Epoch 65/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9752 - acc: 0.7701 - val_loss: 0.6030 - val_acc: 0.8115\n",
      "Epoch 66/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9727 - acc: 0.7723 - val_loss: 0.6252 - val_acc: 0.8085\n",
      "Epoch 67/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9736 - acc: 0.7712 - val_loss: 0.5891 - val_acc: 0.8105\n",
      "Epoch 68/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9583 - acc: 0.7800 - val_loss: 0.5837 - val_acc: 0.8161\n",
      "Epoch 69/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 0.9572 - acc: 0.7774 - val_loss: 0.5859 - val_acc: 0.8177\n",
      "Epoch 70/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 0.9620 - acc: 0.7792 - val_loss: 0.6165 - val_acc: 0.8084\n",
      "Epoch 71/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9594 - acc: 0.7769 - val_loss: 0.5652 - val_acc: 0.8209\n",
      "Epoch 72/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9527 - acc: 0.7798 - val_loss: 0.5743 - val_acc: 0.8214\n",
      "Epoch 73/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9502 - acc: 0.7818 - val_loss: 0.5865 - val_acc: 0.8132\n",
      "Epoch 74/75\n",
      "97/97 [==============================] - 60s 619ms/step - loss: 0.9434 - acc: 0.7861 - val_loss: 0.5778 - val_acc: 0.8163\n",
      "Epoch 75/75\n",
      "97/97 [==============================] - 60s 620ms/step - loss: 0.9399 - acc: 0.7864 - val_loss: 0.5257 - val_acc: 0.8277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0bd8259438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr_triangular = CyclicLR(base_lr=0.1, max_lr=4.0,step_size=3662, mode='triangular')\n",
    "\n",
    "\n",
    "#May be near to https://arxiv.org/abs/1506.01186 (with a single cycle (a.k.a 1-Cycle policy)) using call backs. Need to try and then verify.\n",
    "#model.fit_generator(datagen_k.flow(x_train, y_train,  batch_size=batch_size), steps_per_epoch = np.ceil(len(x_train)//batch_size), verbose=1, epochs=epochs, validation_data=(x_test, y_test), callbacks=[clr_triangular])\n",
    "\n",
    "#Using both mix up and single cycle (a.k.a 1-Cycle policy)\n",
    "model.fit_generator(generator = datagen_k, steps_per_epoch = np.ceil(len(x_train)//batch_size), verbose=1, epochs=epochs, validation_data=(x_test, y_test), callbacks=[clr_triangular])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "Rhetxxo0UtnL",
    "outputId": "a2758529-63b5-4e36-ad38-b39c2b134850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0c25a5c390>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "e8pmPlMGUtnN",
    "outputId": "b7160f6f-7b99-4707-ac8b-51c071853ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 590us/step\n",
      "Test loss: 0.5257053505420685\n",
      "Test accuracy: 0.8277\n"
     ]
    }
   ],
   "source": [
    "# Test the model - 1st time with reduced image size\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wuF_fidyUtnQ",
    "outputId": "4bb244d6-df38-4149-eee8-5f2163f7b92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model_75_epochs_size_16_channel_preprocess_sc_mixup.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dDeP5EDQUtnT",
    "outputId": "626547d1-627f-4647-b696-9c8a0b0c4b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from disk\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"DNST_model_75_epochs_size_16_channel_preprocess_sc_mixup.h5\")\n",
    "print(\"load model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWk3EJqEUtnU"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data - Original Size\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "mean_cifar10 = [125.3, 123.0, 113.9]\n",
    "std_cifar10  = [63.0,  62.1,  66.7]\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#Perchannel normalization\n",
    "def defnormalize_image_by_chanel(img):\n",
    "    new_img = np.zeros(img.shape)\n",
    "    mean_cifar10 = [125.3, 123.0, 113.9]\n",
    "    std_cifar10  = [63.0,  62.1,  66.7]\n",
    "    for i in range(3):\n",
    "        new_img[:, :, i] = (img[:, :, i] - mean_cifar10[i]) / std_cifar10[i]\n",
    "    return new_img\n",
    "\n",
    "x_train = np.asarray([defnormalize_image_by_chanel(image) for image in x_train])\n",
    "x_test = np.asarray([defnormalize_image_by_chanel(image) for image in x_test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "datagen_k = MixupGenerator(x_train, y_train,  batch_size=batch_size, alpha=0.4, datagen=datagen_bm)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GU0j2U2CUtnX"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1734
    },
    "colab_type": "code",
    "id": "dhFJVTDTUtnZ",
    "outputId": "3aa98a12-490e-4206-ea8b-d63aba673618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "195/195 [==============================] - 79s 406ms/step - loss: 1.1131 - acc: 0.7313 - val_loss: 0.6798 - val_acc: 0.7855\n",
      "Epoch 2/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 1.0509 - acc: 0.7566 - val_loss: 0.5814 - val_acc: 0.8171\n",
      "Epoch 3/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 1.0293 - acc: 0.7696 - val_loss: 0.5640 - val_acc: 0.8197\n",
      "Epoch 4/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 1.0074 - acc: 0.7781 - val_loss: 0.5227 - val_acc: 0.8323\n",
      "Epoch 5/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9950 - acc: 0.7826 - val_loss: 0.4550 - val_acc: 0.8545\n",
      "Epoch 6/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9880 - acc: 0.7864 - val_loss: 0.5012 - val_acc: 0.8394\n",
      "Epoch 7/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9802 - acc: 0.7923 - val_loss: 0.5411 - val_acc: 0.8290\n",
      "Epoch 8/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9760 - acc: 0.7917 - val_loss: 0.5344 - val_acc: 0.8376\n",
      "Epoch 9/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9730 - acc: 0.7926 - val_loss: 0.4237 - val_acc: 0.8690\n",
      "Epoch 10/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9647 - acc: 0.7987 - val_loss: 0.5372 - val_acc: 0.8348\n",
      "Epoch 11/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9672 - acc: 0.7979 - val_loss: 0.5687 - val_acc: 0.8241\n",
      "Epoch 12/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9627 - acc: 0.8008 - val_loss: 0.4978 - val_acc: 0.8462\n",
      "Epoch 13/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9620 - acc: 0.7983 - val_loss: 0.5052 - val_acc: 0.8490\n",
      "Epoch 14/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9603 - acc: 0.8011 - val_loss: 0.5606 - val_acc: 0.8284\n",
      "Epoch 15/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9599 - acc: 0.8023 - val_loss: 0.4540 - val_acc: 0.8592\n",
      "Epoch 16/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9492 - acc: 0.8063 - val_loss: 0.5513 - val_acc: 0.8336\n",
      "Epoch 17/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9572 - acc: 0.8030 - val_loss: 0.4920 - val_acc: 0.8445\n",
      "Epoch 18/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9539 - acc: 0.8051 - val_loss: 0.5185 - val_acc: 0.8410\n",
      "Epoch 19/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9576 - acc: 0.8008 - val_loss: 0.4684 - val_acc: 0.8504\n",
      "Epoch 20/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9482 - acc: 0.8063 - val_loss: 0.5575 - val_acc: 0.8357\n",
      "Epoch 21/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9532 - acc: 0.8029 - val_loss: 0.6075 - val_acc: 0.8212\n",
      "Epoch 22/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9548 - acc: 0.8035 - val_loss: 0.5392 - val_acc: 0.8387\n",
      "Epoch 23/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9484 - acc: 0.8054 - val_loss: 0.5172 - val_acc: 0.8349\n",
      "Epoch 24/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9439 - acc: 0.8070 - val_loss: 0.4643 - val_acc: 0.8599\n",
      "Epoch 25/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9520 - acc: 0.8027 - val_loss: 0.4742 - val_acc: 0.8566\n",
      "Epoch 26/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9518 - acc: 0.8048 - val_loss: 0.3962 - val_acc: 0.8751\n",
      "Epoch 27/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9482 - acc: 0.8078 - val_loss: 0.5783 - val_acc: 0.8191\n",
      "Epoch 28/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9500 - acc: 0.8063 - val_loss: 0.6292 - val_acc: 0.8184\n",
      "Epoch 29/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.9442 - acc: 0.8089 - val_loss: 0.5363 - val_acc: 0.8315\n",
      "Epoch 30/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9529 - acc: 0.8058 - val_loss: 0.5658 - val_acc: 0.8254\n",
      "Epoch 31/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.9395 - acc: 0.8096 - val_loss: 0.6157 - val_acc: 0.8293\n",
      "Epoch 32/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9449 - acc: 0.8110 - val_loss: 0.5399 - val_acc: 0.8344\n",
      "Epoch 33/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9385 - acc: 0.8114 - val_loss: 0.4641 - val_acc: 0.8563\n",
      "Epoch 34/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9411 - acc: 0.8111 - val_loss: 0.6659 - val_acc: 0.8032\n",
      "Epoch 35/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9461 - acc: 0.8073 - val_loss: 0.4019 - val_acc: 0.8733\n",
      "Epoch 36/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9446 - acc: 0.8085 - val_loss: 0.5115 - val_acc: 0.8411\n",
      "Epoch 37/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9536 - acc: 0.8062 - val_loss: 0.4710 - val_acc: 0.8553\n",
      "Epoch 38/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9442 - acc: 0.8093 - val_loss: 0.5727 - val_acc: 0.8343\n",
      "Epoch 39/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.9425 - acc: 0.8102 - val_loss: 0.4387 - val_acc: 0.8646\n",
      "Epoch 40/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9372 - acc: 0.8125 - val_loss: 0.4701 - val_acc: 0.8537\n",
      "Epoch 41/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9409 - acc: 0.8096 - val_loss: 0.4279 - val_acc: 0.8660\n",
      "Epoch 42/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9394 - acc: 0.8143 - val_loss: 0.6095 - val_acc: 0.8215\n",
      "Epoch 43/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9422 - acc: 0.8093 - val_loss: 0.5245 - val_acc: 0.8447\n",
      "Epoch 44/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9386 - acc: 0.8095 - val_loss: 0.5209 - val_acc: 0.8436\n",
      "Epoch 45/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9385 - acc: 0.8113 - val_loss: 0.5540 - val_acc: 0.8296\n",
      "Epoch 46/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9420 - acc: 0.8106 - val_loss: 0.4278 - val_acc: 0.8679\n",
      "Epoch 47/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9435 - acc: 0.8106 - val_loss: 0.4817 - val_acc: 0.8573\n",
      "Epoch 48/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.9437 - acc: 0.8108 - val_loss: 0.7501 - val_acc: 0.7839\n",
      "Epoch 49/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9431 - acc: 0.8100 - val_loss: 0.5454 - val_acc: 0.8400\n",
      "Epoch 50/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9351 - acc: 0.8139 - val_loss: 0.6692 - val_acc: 0.8008\n",
      "Epoch 51/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.9399 - acc: 0.8138 - val_loss: 0.7048 - val_acc: 0.7948\n",
      "Epoch 52/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9418 - acc: 0.8131 - val_loss: 0.4994 - val_acc: 0.8443\n",
      "Epoch 53/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9342 - acc: 0.8154 - val_loss: 0.6373 - val_acc: 0.8160\n",
      "Epoch 54/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9361 - acc: 0.8149 - val_loss: 0.6197 - val_acc: 0.8218\n",
      "Epoch 55/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9352 - acc: 0.8130 - val_loss: 0.4895 - val_acc: 0.8505\n",
      "Epoch 56/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9309 - acc: 0.8173 - val_loss: 0.4329 - val_acc: 0.8630\n",
      "Epoch 57/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9263 - acc: 0.8172 - val_loss: 0.5407 - val_acc: 0.8333\n",
      "Epoch 58/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9392 - acc: 0.8143 - val_loss: 0.7162 - val_acc: 0.7864\n",
      "Epoch 59/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9306 - acc: 0.8180 - val_loss: 0.4917 - val_acc: 0.8513\n",
      "Epoch 60/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9303 - acc: 0.8149 - val_loss: 0.5058 - val_acc: 0.8559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9296 - acc: 0.8175 - val_loss: 0.8028 - val_acc: 0.7792\n",
      "Epoch 62/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9358 - acc: 0.8135 - val_loss: 0.5357 - val_acc: 0.8472\n",
      "Epoch 63/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9316 - acc: 0.8178 - val_loss: 0.5811 - val_acc: 0.8246\n",
      "Epoch 64/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9289 - acc: 0.8174 - val_loss: 0.4938 - val_acc: 0.8566\n",
      "Epoch 65/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9340 - acc: 0.8148 - val_loss: 0.4604 - val_acc: 0.8596\n",
      "Epoch 66/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9234 - acc: 0.8204 - val_loss: 0.5532 - val_acc: 0.8343\n",
      "Epoch 67/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9309 - acc: 0.8195 - val_loss: 0.6078 - val_acc: 0.8218\n",
      "Epoch 68/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9245 - acc: 0.8187 - val_loss: 0.5006 - val_acc: 0.8522\n",
      "Epoch 69/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9236 - acc: 0.8200 - val_loss: 0.6543 - val_acc: 0.8201\n",
      "Epoch 70/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9285 - acc: 0.8167 - val_loss: 0.4712 - val_acc: 0.8552\n",
      "Epoch 71/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.9283 - acc: 0.8194 - val_loss: 0.5480 - val_acc: 0.8297\n",
      "Epoch 72/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9296 - acc: 0.8188 - val_loss: 0.5675 - val_acc: 0.8425\n",
      "Epoch 73/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9228 - acc: 0.8192 - val_loss: 0.6020 - val_acc: 0.8203\n",
      "Epoch 74/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9272 - acc: 0.8185 - val_loss: 0.6087 - val_acc: 0.8319\n",
      "Epoch 75/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9256 - acc: 0.8208 - val_loss: 0.3840 - val_acc: 0.8847\n",
      "Epoch 76/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9223 - acc: 0.8216 - val_loss: 0.4611 - val_acc: 0.8573\n",
      "Epoch 77/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.9204 - acc: 0.8217 - val_loss: 0.6040 - val_acc: 0.8249\n",
      "Epoch 78/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.9180 - acc: 0.8234 - val_loss: 0.4473 - val_acc: 0.8612\n",
      "Epoch 79/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9086 - acc: 0.8272 - val_loss: 0.4912 - val_acc: 0.8585\n",
      "Epoch 80/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.9091 - acc: 0.8239 - val_loss: 0.4958 - val_acc: 0.8443\n",
      "Epoch 81/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.9066 - acc: 0.8286 - val_loss: 0.5913 - val_acc: 0.8256\n",
      "Epoch 82/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.9005 - acc: 0.8283 - val_loss: 0.4325 - val_acc: 0.8686\n",
      "Epoch 83/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8966 - acc: 0.8309 - val_loss: 0.3669 - val_acc: 0.8867\n",
      "Epoch 84/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8987 - acc: 0.8282 - val_loss: 0.4409 - val_acc: 0.8667\n",
      "Epoch 85/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8938 - acc: 0.8336 - val_loss: 0.5187 - val_acc: 0.8601\n",
      "Epoch 86/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8890 - acc: 0.8333 - val_loss: 0.4498 - val_acc: 0.8609\n",
      "Epoch 87/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8913 - acc: 0.8322 - val_loss: 0.4775 - val_acc: 0.8582\n",
      "Epoch 88/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8966 - acc: 0.8321 - val_loss: 0.6217 - val_acc: 0.8203\n",
      "Epoch 89/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.8852 - acc: 0.8336 - val_loss: 0.5638 - val_acc: 0.8424\n",
      "Epoch 90/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8851 - acc: 0.8364 - val_loss: 0.6087 - val_acc: 0.8262\n",
      "Epoch 91/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8786 - acc: 0.8361 - val_loss: 0.4717 - val_acc: 0.8678\n",
      "Epoch 92/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8779 - acc: 0.8379 - val_loss: 0.4288 - val_acc: 0.8717\n",
      "Epoch 93/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.8787 - acc: 0.8378 - val_loss: 0.5120 - val_acc: 0.8486\n",
      "Epoch 94/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8731 - acc: 0.8395 - val_loss: 0.3831 - val_acc: 0.8838\n",
      "Epoch 95/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8698 - acc: 0.8412 - val_loss: 0.4148 - val_acc: 0.8728\n",
      "Epoch 96/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8728 - acc: 0.8398 - val_loss: 0.4273 - val_acc: 0.8771\n",
      "Epoch 97/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.8655 - acc: 0.8436 - val_loss: 0.3964 - val_acc: 0.8812\n",
      "Epoch 98/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8651 - acc: 0.8423 - val_loss: 0.4278 - val_acc: 0.8697\n",
      "Epoch 99/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8617 - acc: 0.8437 - val_loss: 0.4257 - val_acc: 0.8750\n",
      "Epoch 100/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.8646 - acc: 0.8404 - val_loss: 0.2901 - val_acc: 0.9109\n",
      "Epoch 101/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8671 - acc: 0.8426 - val_loss: 0.3798 - val_acc: 0.8901\n",
      "Epoch 102/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.8575 - acc: 0.8452 - val_loss: 0.3328 - val_acc: 0.8979\n",
      "Epoch 103/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8528 - acc: 0.8487 - val_loss: 0.3219 - val_acc: 0.9027\n",
      "Epoch 104/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8578 - acc: 0.8478 - val_loss: 0.2971 - val_acc: 0.9084\n",
      "Epoch 105/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8525 - acc: 0.8486 - val_loss: 0.3981 - val_acc: 0.8882\n",
      "Epoch 106/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8513 - acc: 0.8480 - val_loss: 0.3406 - val_acc: 0.8996\n",
      "Epoch 107/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.8537 - acc: 0.8465 - val_loss: 0.3789 - val_acc: 0.8898\n",
      "Epoch 108/150\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 0.8476 - acc: 0.8501 - val_loss: 0.3620 - val_acc: 0.8973\n",
      "Epoch 109/150\n",
      "195/195 [==============================] - 60s 308ms/step - loss: 0.8392 - acc: 0.8519 - val_loss: 0.3908 - val_acc: 0.8813\n",
      "Epoch 110/150\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 0.8491 - acc: 0.8485 - val_loss: 0.3599 - val_acc: 0.8946\n",
      "Epoch 111/150\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 0.8455 - acc: 0.8490 - val_loss: 0.3795 - val_acc: 0.8904\n",
      "Epoch 112/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8436 - acc: 0.8505 - val_loss: 0.3303 - val_acc: 0.9023\n",
      "Epoch 113/150\n",
      "195/195 [==============================] - 60s 308ms/step - loss: 0.8425 - acc: 0.8503 - val_loss: 0.2982 - val_acc: 0.9095\n",
      "Epoch 114/150\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 0.8348 - acc: 0.8541 - val_loss: 0.3062 - val_acc: 0.9082\n",
      "Epoch 115/150\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 0.8352 - acc: 0.8543 - val_loss: 0.4223 - val_acc: 0.8754\n",
      "Epoch 116/150\n",
      "195/195 [==============================] - 60s 307ms/step - loss: 0.8328 - acc: 0.8552 - val_loss: 0.3903 - val_acc: 0.8847\n",
      "Epoch 117/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8247 - acc: 0.8566 - val_loss: 0.3070 - val_acc: 0.9093\n",
      "Epoch 118/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.8315 - acc: 0.8562 - val_loss: 0.3551 - val_acc: 0.8960\n",
      "Epoch 119/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8208 - acc: 0.8603 - val_loss: 0.4175 - val_acc: 0.8799\n",
      "Epoch 120/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8301 - acc: 0.8560 - val_loss: 0.2876 - val_acc: 0.9139\n",
      "Epoch 121/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8308 - acc: 0.8519 - val_loss: 0.3165 - val_acc: 0.9071\n",
      "Epoch 122/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8214 - acc: 0.8580 - val_loss: 0.2864 - val_acc: 0.9140\n",
      "Epoch 123/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8227 - acc: 0.8593 - val_loss: 0.2814 - val_acc: 0.9178\n",
      "Epoch 124/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8215 - acc: 0.8602 - val_loss: 0.2873 - val_acc: 0.9167\n",
      "Epoch 125/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8150 - acc: 0.8609 - val_loss: 0.3173 - val_acc: 0.9066\n",
      "Epoch 126/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8134 - acc: 0.8616 - val_loss: 0.2657 - val_acc: 0.9212\n",
      "Epoch 127/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8172 - acc: 0.8614 - val_loss: 0.3029 - val_acc: 0.9104\n",
      "Epoch 128/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8161 - acc: 0.8645 - val_loss: 0.2891 - val_acc: 0.9154\n",
      "Epoch 129/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8162 - acc: 0.8608 - val_loss: 0.3882 - val_acc: 0.8916\n",
      "Epoch 130/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8116 - acc: 0.8635 - val_loss: 0.2705 - val_acc: 0.9210\n",
      "Epoch 131/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8149 - acc: 0.8630 - val_loss: 0.3313 - val_acc: 0.9047\n",
      "Epoch 132/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8142 - acc: 0.8638 - val_loss: 0.2765 - val_acc: 0.9194\n",
      "Epoch 133/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.8083 - acc: 0.8621 - val_loss: 0.2661 - val_acc: 0.9221\n",
      "Epoch 134/150\n",
      "195/195 [==============================] - 60s 309ms/step - loss: 0.8016 - acc: 0.8663 - val_loss: 0.3481 - val_acc: 0.9026\n",
      "Epoch 135/150\n",
      "195/195 [==============================] - 61s 310ms/step - loss: 0.8055 - acc: 0.8641 - val_loss: 0.2743 - val_acc: 0.9204\n",
      "Epoch 136/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.8027 - acc: 0.8655 - val_loss: 0.2998 - val_acc: 0.9140\n",
      "Epoch 137/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.8012 - acc: 0.8645 - val_loss: 0.2864 - val_acc: 0.9184\n",
      "Epoch 138/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.7984 - acc: 0.8676 - val_loss: 0.3109 - val_acc: 0.9102\n",
      "Epoch 139/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.7982 - acc: 0.8662 - val_loss: 0.2719 - val_acc: 0.9205\n",
      "Epoch 140/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.7959 - acc: 0.8690 - val_loss: 0.2717 - val_acc: 0.9221\n",
      "Epoch 141/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.7973 - acc: 0.8674 - val_loss: 0.2572 - val_acc: 0.9251\n",
      "Epoch 142/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.7964 - acc: 0.8676 - val_loss: 0.2820 - val_acc: 0.9198\n",
      "Epoch 143/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.7915 - acc: 0.8689 - val_loss: 0.2545 - val_acc: 0.9261\n",
      "Epoch 144/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.7888 - acc: 0.8700 - val_loss: 0.2816 - val_acc: 0.9177\n",
      "Epoch 145/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.7860 - acc: 0.8713 - val_loss: 0.2616 - val_acc: 0.9241\n",
      "Epoch 146/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.7876 - acc: 0.8715 - val_loss: 0.2639 - val_acc: 0.9235\n",
      "Epoch 147/150\n",
      "195/195 [==============================] - 60s 310ms/step - loss: 0.7814 - acc: 0.8741 - val_loss: 0.2659 - val_acc: 0.9228\n",
      "Epoch 148/150\n",
      "195/195 [==============================] - 61s 313ms/step - loss: 0.7851 - acc: 0.8712 - val_loss: 0.2561 - val_acc: 0.9251\n",
      "Epoch 149/150\n",
      "195/195 [==============================] - 61s 311ms/step - loss: 0.7858 - acc: 0.8704 - val_loss: 0.2586 - val_acc: 0.9250\n",
      "Epoch 150/150\n",
      "195/195 [==============================] - 61s 312ms/step - loss: 0.7764 - acc: 0.8745 - val_loss: 0.2641 - val_acc: 0.9238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7de90adac8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 150\n",
    "\n",
    "s_size = np.ceil((((len(x_train)/batch_size)) * epochs)/2)\n",
    "\n",
    "clr_triangular = CyclicLR(base_lr=0.1, max_lr=4.0,step_size=s_size, mode='triangular')\n",
    "\n",
    "\n",
    "#May be near to https://arxiv.org/abs/1506.01186 (with a single cycle (a.k.a 1-Cycle policy)) using call backs. Need to try and then verify.\n",
    "#model.fit_generator(datagen_k.flow(x_train, y_train,  batch_size=batch_size), steps_per_epoch = np.ceil(len(x_train)//batch_size), verbose=1, epochs=epochs, validation_data=(x_test, y_test), callbacks=[clr_triangular])\n",
    "\n",
    "#Using both mix up and single cycle (a.k.a 1-Cycle policy)\n",
    "model.fit_generator(generator = datagen_k, steps_per_epoch = np.ceil(len(x_train)//batch_size), verbose=1, epochs=epochs, validation_data=(x_test, y_test), callbacks=[clr_triangular])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "048Ow_dVUtnd",
    "outputId": "4f7a99dc-fa40-4e3c-8b86-0618a93886f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7df6e45d68>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "1e1f12f7-4a00-48f0-dde4-3eef8c5cbbca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 532us/step\n",
      "Test loss: 0.2640875468611717\n",
      "Test accuracy: 0.9238\n"
     ]
    }
   ],
   "source": [
    "# Test the model - 32x32 Image Size\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H5trmxw4Utnl",
    "outputId": "e0c393a2-8fe6-4585-84be-3ed106d0313b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model_150_epochs_size_32_channel_preprocess_sc_mixup.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tzpk84wAUtno"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai-yZ2ED5AK1"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('DNST_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SC_50_Normalized_DNST_CIFAR10_AUG_DiffNet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
